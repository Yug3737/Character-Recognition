{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yug3737/Character-Recognition/blob/main/Digit_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the input data to [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = np.eye(10)[y_train]\n",
        "y_test = np.eye(10)[y_test]\n",
        "\n",
        "# Flatten the images\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "# Activation functions and derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability adjustment\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Neural Network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.weights1 = np.random.randn(input_size, hidden_sizes[0]) * 0.01\n",
        "        self.bias1 = np.zeros((1, hidden_sizes[0]))\n",
        "\n",
        "        self.weights2 = np.random.randn(hidden_sizes[0], hidden_sizes[1]) * 0.01\n",
        "        self.bias2 = np.zeros((1, hidden_sizes[1]))\n",
        "\n",
        "        self.weights3 = np.random.randn(hidden_sizes[1], output_size) * 0.01\n",
        "        self.bias3 = np.zeros((1, output_size))\n",
        "\n",
        "        self.learning_rate = 0.01\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, X):\n",
        "        self.preact1 = np.dot(X, self.weights1) + self.bias1\n",
        "        self.postact1 = relu(self.preact1)\n",
        "\n",
        "        self.preact2 = np.dot(self.postact1, self.weights2) + self.bias2\n",
        "        self.postact2 = relu(self.preact2)\n",
        "\n",
        "        self.preact3 = np.dot(self.postact2, self.weights3) + self.bias3\n",
        "        self.postact3 = softmax(self.preact3)\n",
        "\n",
        "        return self.postact3\n",
        "\n",
        "    # Backpropagation\n",
        "    def backward(self, X, y, y_pred):\n",
        "        m = y.shape[0]  # Number of samples\n",
        "\n",
        "        # Gradients for the output layer\n",
        "        d_preact3 = y_pred - y\n",
        "        d_weights3 = np.dot(self.postact2.T, d_preact3) / m\n",
        "        d_bias3 = np.sum(d_preact3, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Gradients for the second hidden layer\n",
        "        d_postact2 = np.dot(d_preact3, self.weights3.T)\n",
        "        d_preact2 = d_postact2 * relu_derivative(self.preact2)\n",
        "        d_weights2 = np.dot(self.postact1.T, d_preact2) / m\n",
        "        d_bias2 = np.sum(d_preact2, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Gradients for the first hidden layer\n",
        "        d_postact1 = np.dot(d_preact2, self.weights2.T)\n",
        "        d_preact1 = d_postact1 * relu_derivative(self.preact1)\n",
        "        d_weights1 = np.dot(X.T, d_preact1) / m\n",
        "        d_bias1 = np.sum(d_preact1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights3 -= self.learning_rate * d_weights3\n",
        "        self.bias3 -= self.learning_rate * d_bias3\n",
        "        self.weights2 -= self.learning_rate * d_weights2\n",
        "        self.bias2 -= self.learning_rate * d_bias2\n",
        "        self.weights1 -= self.learning_rate * d_weights1\n",
        "        self.bias1 -= self.learning_rate * d_bias1\n",
        "\n",
        "    # Training function\n",
        "    def train(self, X, y, epochs, batch_size):\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle data\n",
        "            indices = np.arange(X.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X[i:i + batch_size]\n",
        "                y_batch = y[i:i + batch_size]\n",
        "\n",
        "                # Forward and backward propagation\n",
        "                y_pred = self.forward(X_batch)\n",
        "                self.backward(X_batch, y_batch, y_pred)\n",
        "\n",
        "            # Compute loss for the epoch\n",
        "            y_pred = self.forward(X)\n",
        "            loss = -np.mean(np.sum(y * np.log(y_pred + 1e-8), axis=1))  # Categorical cross-entropy\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # Predict function\n",
        "    def predict(self, X):\n",
        "        y_pred = self.forward(X)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Initialize and train the network\n",
        "input_size = 784  # Flattened image size\n",
        "hidden_sizes = [128, 64]  # Number of neurons in hidden layers\n",
        "output_size = 10# Number of output classes\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "nn = NeuralNetwork(input_size, hidden_sizes, output_size)\n",
        "nn.train(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_test_pred = nn.predict(x_test)\n",
        "y_test_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_test_pred == y_test_true)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtJ9WLiNMK6_",
        "outputId": "5eca8469-b961-4f25-b04e-9ed4fad7a31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 2.3009\n",
            "Epoch 2/100, Loss: 2.2992\n",
            "Epoch 3/100, Loss: 2.2892\n",
            "Epoch 4/100, Loss: 1.8888\n",
            "Epoch 5/100, Loss: 0.7884\n",
            "Epoch 6/100, Loss: 0.6371\n",
            "Epoch 7/100, Loss: 0.5636\n",
            "Epoch 8/100, Loss: 0.5034\n",
            "Epoch 9/100, Loss: 0.4621\n",
            "Epoch 10/100, Loss: 0.4173\n",
            "Epoch 11/100, Loss: 0.3852\n",
            "Epoch 12/100, Loss: 0.3548\n",
            "Epoch 13/100, Loss: 0.3306\n",
            "Epoch 14/100, Loss: 0.3066\n",
            "Epoch 15/100, Loss: 0.2867\n",
            "Epoch 16/100, Loss: 0.2664\n",
            "Epoch 17/100, Loss: 0.2497\n",
            "Epoch 18/100, Loss: 0.2348\n",
            "Epoch 19/100, Loss: 0.2195\n",
            "Epoch 20/100, Loss: 0.2071\n",
            "Epoch 21/100, Loss: 0.1967\n",
            "Epoch 22/100, Loss: 0.1862\n",
            "Epoch 23/100, Loss: 0.1797\n",
            "Epoch 24/100, Loss: 0.1755\n",
            "Epoch 25/100, Loss: 0.1635\n",
            "Epoch 26/100, Loss: 0.1579\n",
            "Epoch 27/100, Loss: 0.1507\n",
            "Epoch 28/100, Loss: 0.1436\n",
            "Epoch 29/100, Loss: 0.1410\n",
            "Epoch 30/100, Loss: 0.1338\n",
            "Epoch 31/100, Loss: 0.1286\n",
            "Epoch 32/100, Loss: 0.1236\n",
            "Epoch 33/100, Loss: 0.1213\n",
            "Epoch 34/100, Loss: 0.1162\n",
            "Epoch 35/100, Loss: 0.1139\n",
            "Epoch 36/100, Loss: 0.1096\n",
            "Epoch 37/100, Loss: 0.1063\n",
            "Epoch 38/100, Loss: 0.1015\n",
            "Epoch 39/100, Loss: 0.0999\n",
            "Epoch 40/100, Loss: 0.0965\n",
            "Epoch 41/100, Loss: 0.0964\n",
            "Epoch 42/100, Loss: 0.0902\n",
            "Epoch 43/100, Loss: 0.0907\n",
            "Epoch 44/100, Loss: 0.0848\n",
            "Epoch 45/100, Loss: 0.0821\n",
            "Epoch 46/100, Loss: 0.0795\n",
            "Epoch 47/100, Loss: 0.0795\n",
            "Epoch 48/100, Loss: 0.0778\n",
            "Epoch 49/100, Loss: 0.0742\n",
            "Epoch 50/100, Loss: 0.0712\n",
            "Epoch 51/100, Loss: 0.0720\n",
            "Epoch 52/100, Loss: 0.0677\n",
            "Epoch 53/100, Loss: 0.0665\n",
            "Epoch 54/100, Loss: 0.0647\n",
            "Epoch 55/100, Loss: 0.0619\n",
            "Epoch 56/100, Loss: 0.0629\n",
            "Epoch 57/100, Loss: 0.0601\n",
            "Epoch 58/100, Loss: 0.0585\n",
            "Epoch 59/100, Loss: 0.0561\n",
            "Epoch 60/100, Loss: 0.0558\n",
            "Epoch 61/100, Loss: 0.0547\n",
            "Epoch 62/100, Loss: 0.0517\n",
            "Epoch 63/100, Loss: 0.0521\n",
            "Epoch 64/100, Loss: 0.0497\n",
            "Epoch 65/100, Loss: 0.0481\n",
            "Epoch 66/100, Loss: 0.0467\n",
            "Epoch 67/100, Loss: 0.0464\n",
            "Epoch 68/100, Loss: 0.0443\n",
            "Epoch 69/100, Loss: 0.0448\n",
            "Epoch 70/100, Loss: 0.0434\n",
            "Epoch 71/100, Loss: 0.0415\n",
            "Epoch 72/100, Loss: 0.0440\n",
            "Epoch 73/100, Loss: 0.0396\n",
            "Epoch 74/100, Loss: 0.0388\n",
            "Epoch 75/100, Loss: 0.0384\n",
            "Epoch 76/100, Loss: 0.0381\n",
            "Epoch 77/100, Loss: 0.0363\n",
            "Epoch 78/100, Loss: 0.0350\n",
            "Epoch 79/100, Loss: 0.0342\n",
            "Epoch 80/100, Loss: 0.0343\n",
            "Epoch 81/100, Loss: 0.0337\n",
            "Epoch 82/100, Loss: 0.0323\n",
            "Epoch 83/100, Loss: 0.0316\n",
            "Epoch 84/100, Loss: 0.0312\n",
            "Epoch 85/100, Loss: 0.0301\n",
            "Epoch 86/100, Loss: 0.0295\n",
            "Epoch 87/100, Loss: 0.0285\n",
            "Epoch 88/100, Loss: 0.0290\n",
            "Epoch 89/100, Loss: 0.0273\n",
            "Epoch 90/100, Loss: 0.0273\n",
            "Epoch 91/100, Loss: 0.0265\n",
            "Epoch 92/100, Loss: 0.0256\n",
            "Epoch 93/100, Loss: 0.0254\n",
            "Epoch 94/100, Loss: 0.0247\n",
            "Epoch 95/100, Loss: 0.0236\n",
            "Epoch 96/100, Loss: 0.0233\n",
            "Epoch 97/100, Loss: 0.0229\n",
            "Epoch 98/100, Loss: 0.0226\n",
            "Epoch 99/100, Loss: 0.0220\n",
            "Epoch 100/100, Loss: 0.0215\n",
            "Test Accuracy: 97.54%\n"
          ]
        }
      ]
    }
  ]
}