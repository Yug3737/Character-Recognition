{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNgyGuKoLG4DqOBtwcZgxTS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yug3737/Character-Recognition/blob/main/Letter_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHd2wl6hWSix",
        "outputId": "6384c594-39fb-4136-cb4c-af3cbcd681a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 562M/562M [00:09<00:00, 60.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n",
            "(<PIL.Image.Image image mode=L size=28x28 at 0x7FAAF8505090>, 35)\n",
            "<class 'tuple'>\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import EMNIST\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "\n",
        "emnist_data=EMNIST(root=\"./data\",split=\"byclass\",download=True)\n",
        "\n",
        "print(emnist_data[0])\n",
        "print(type(emnist_data[0]))\n",
        "\n",
        "from collections import Counter\n",
        "data_freq=Counter(labels)\n",
        "data_freq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# image = images[0]\n",
        "# image = cv2.imdecode(image, cv2.IMREAD_GRAYSCALE)\n",
        "# cv2_imshow(images[0])\n",
        "\n",
        "from collections import defaultdict\n",
        "images = []\n",
        "labels = []\n",
        "labels_count=defaultdict(int)\n",
        "for i in range(len(emnist_data)):\n",
        "  if labels_count[emnist_data[i][1]]<4000:\n",
        "    labels_count[emnist_data[i][1]]+=1\n",
        "    images.append(emnist_data[i][0])\n",
        "    labels.append(emnist_data[i][1])\n",
        ""
      ],
      "metadata": {
        "id": "V2fADrnuX4et"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10Uoq76OcGDn",
        "outputId": "ea69b2dc-574f-4c7a-b4ad-a9fe25718a41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "217952"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "#labels_copy=labels\n",
        "data_freq=Counter(labels)\n",
        "data_freq\n",
        "\n",
        "# Get min value and only keep that value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M20bQAeZZICo",
        "outputId": "94467c89-9e40-401c-c3c1-e3916266dc48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({35: 2701,\n",
              "         36: 4000,\n",
              "         6: 4000,\n",
              "         3: 4000,\n",
              "         22: 4000,\n",
              "         38: 2854,\n",
              "         5: 4000,\n",
              "         9: 4000,\n",
              "         47: 4000,\n",
              "         4: 4000,\n",
              "         7: 4000,\n",
              "         56: 2830,\n",
              "         29: 4000,\n",
              "         40: 4000,\n",
              "         55: 4000,\n",
              "         57: 2910,\n",
              "         43: 4000,\n",
              "         32: 4000,\n",
              "         15: 4000,\n",
              "         26: 2605,\n",
              "         2: 4000,\n",
              "         28: 4000,\n",
              "         8: 4000,\n",
              "         49: 4000,\n",
              "         12: 4000,\n",
              "         1: 4000,\n",
              "         59: 2822,\n",
              "         18: 4000,\n",
              "         41: 2561,\n",
              "         0: 4000,\n",
              "         46: 2491,\n",
              "         23: 4000,\n",
              "         51: 2448,\n",
              "         24: 4000,\n",
              "         31: 4000,\n",
              "         45: 1896,\n",
              "         39: 4000,\n",
              "         11: 3878,\n",
              "         54: 2699,\n",
              "         19: 3762,\n",
              "         20: 2468,\n",
              "         61: 2725,\n",
              "         25: 4000,\n",
              "         37: 4000,\n",
              "         52: 2994,\n",
              "         58: 2697,\n",
              "         14: 4000,\n",
              "         34: 4000,\n",
              "         30: 4000,\n",
              "         27: 4000,\n",
              "         16: 2517,\n",
              "         33: 2771,\n",
              "         21: 4000,\n",
              "         60: 2365,\n",
              "         48: 2645,\n",
              "         53: 4000,\n",
              "         13: 4000,\n",
              "         10: 4000,\n",
              "         50: 2749,\n",
              "         17: 3152,\n",
              "         42: 3687,\n",
              "         44: 2725})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(labels))\n",
        "print(max(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fz_b4XYWwDG",
        "outputId": "112d2301-fa95-495c-f69f-dfeb10ea32a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_images=[]\n",
        "filtered_labels=[]\n",
        "\n",
        "for i in range(len(labels)):\n",
        "  if labels[i]>=10 and labels[i]<=61:\n",
        "    filtered_images.append(images[i])\n",
        "    filtered_labels.append(labels[i])\n",
        "\n",
        "print(min(filtered_labels))\n",
        "print(max(filtered_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRkMW7y8Wze1",
        "outputId": "ae6d96d8-5a29-44bf-e4ed-09a3296a9adb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array(filtered_images)\n",
        "# print(x[:5])\n",
        "# print(y[:5])\n",
        "y=np.array(filtered_labels)\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NprGGYWHW4V0",
        "outputId": "280b38ff-07d3-4c99-a710-c1f171d9d279"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(177952, 28, 28)\n",
            "(177952,)\n",
            "(177952, 28, 28)\n",
            "(177952,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=x.reshape(x.shape[0],784)\n",
        "y=np.eye(52)[y-10]\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=42)\n",
        "\n",
        "print(\"Training Data Size:\", len(x_train))\n",
        "print(\"Testing data size:\", len(x_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvyZOf8CW-zu",
        "outputId": "b6a1588d-07fe-4681-fc75-7777e40cb58d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Size: 160156\n",
            "Testing data size: 17796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    # print(\"x = \", x)\n",
        "    # print(\"np.max(x, axis=1, kdims=t) = \", np.max(x, axis=1, keepdims=True))\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability adjustment\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Neural Network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.weights1 = np.random.randn(input_size, hidden_sizes[0]) * 0.01\n",
        "        self.bias1 = np.zeros((1, hidden_sizes[0]))\n",
        "\n",
        "        self.weights2 = np.random.randn(hidden_sizes[0], hidden_sizes[1]) * 0.01\n",
        "        self.bias2 = np.zeros((1, hidden_sizes[1]))\n",
        "\n",
        "        self.weights3 = np.random.randn(hidden_sizes[1], hidden_sizes[2]) * 0.01\n",
        "        self.bias3 = np.zeros((1, hidden_sizes[2]))\n",
        "\n",
        "        self.weights4 = np.random.randn(hidden_sizes[2], hidden_sizes[3]) * 0.01\n",
        "        self.bias4 = np.zeros((1, hidden_sizes[3]))\n",
        "\n",
        "        self.weights5 = np.random.randn(hidden_sizes[3], output_size) * 0.01\n",
        "        self.bias5 = np.zeros((1, output_size))\n",
        "\n",
        "        self.learning_rate = 0.001\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
        "        self.a1 = relu(self.z1)\n",
        "\n",
        "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
        "        self.a2 = relu(self.z2)\n",
        "\n",
        "        self.z3 = np.dot(self.a2, self.weights3) + self.bias3\n",
        "        self.a3 = relu(self.z3)\n",
        "\n",
        "        self.z4 = np.dot(self.a3, self.weights4) + self.bias4\n",
        "        self.a4 = relu(self.z4)\n",
        "\n",
        "        self.z5 = np.dot(self.a4, self.weights5) + self.bias5\n",
        "        self.a5 = softmax(self.z5)\n",
        "\n",
        "        return self.a5\n",
        "\n",
        "    # Backpropagation\n",
        "    def backward(self, X, y, y_pred):\n",
        "        m = y.shape[0]  # Number of samples\n",
        "\n",
        "\n",
        "        # Gradients for the output layer\n",
        "        # print(\"ypred shape\", y_pred.shape)\n",
        "        # print(\"y shape\", y.shape)\n",
        "        d_z5 = y_pred - y\n",
        "\n",
        "        d_weights5 = np.dot(self.a4.T, d_z5) / m\n",
        "        d_bias5 = np.sum(d_z5, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Gradients for the 4th hidden layer\n",
        "        d_a4 = np.dot(d_z5, self.weights5.T)\n",
        "        d_z4 = d_a4 * relu_derivative(self.z4)\n",
        "        d_weights4 = np.dot(self.a3.T, d_z4) / m\n",
        "        d_bias4 = np.sum(d_z4, axis=0, keepdims=True) / m\n",
        "\n",
        "\n",
        "        # Gradients for 3rd hidden layer\n",
        "        d_a3 = np.dot(d_z4, self.weights4.T)\n",
        "        d_z3 = d_a3 * relu_derivative(self.z3)\n",
        "        d_weights3 = np.dot(self.a2.T, d_z3) / m\n",
        "        d_bias3 = np.sum(d_z3, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Gradients for the second hidden layer\n",
        "        d_a2 = np.dot(d_z3, self.weights3.T)\n",
        "        d_z2 = d_a2 * relu_derivative(self.z2)\n",
        "        d_weights2 = np.dot(self.a1.T, d_z2) / m\n",
        "        d_bias2 = np.sum(d_z2, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Gradients for the first hidden layer\n",
        "        d_a1 = np.dot(d_z2, self.weights2.T)\n",
        "        d_z1 = d_a1 * relu_derivative(self.z1)\n",
        "        d_weights1 = np.dot(X.T, d_z1) / m\n",
        "        d_bias1 = np.sum(d_z1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights5   -= self.learning_rate * d_weights5\n",
        "        self.bias5      -= self.learning_rate * d_bias5\n",
        "        self.weights4   -= self.learning_rate * d_weights4\n",
        "        self.bias4      -= self.learning_rate * d_bias4\n",
        "        self.weights3   -= self.learning_rate * d_weights3\n",
        "        self.bias3      -= self.learning_rate * d_bias3\n",
        "        self.weights2   -= self.learning_rate * d_weights2\n",
        "        self.bias2      -= self.learning_rate * d_bias2\n",
        "        self.weights1   -= self.learning_rate * d_weights1\n",
        "        self.bias1      -= self.learning_rate * d_bias1\n",
        "\n",
        "    # Training function\n",
        "    def train(self, X, y, epochs, batch_size):\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle data\n",
        "            indices = np.arange(X.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X[i:i + batch_size]\n",
        "                y_batch = y[i:i + batch_size]\n",
        "\n",
        "                # Forward and backward propagation\n",
        "                y_pred = self.forward(X_batch)\n",
        "                self.backward(X_batch, y_batch, y_pred)\n",
        "\n",
        "            # Compute loss for the epoch\n",
        "            y_pred = self.forward(X)\n",
        "            # loss = -np.mean(np.sum(y * np.log(np.clip(y_pred, 1e-8, 1.0)), axis=1))  # Categorical cross-entropy\n",
        "            loss = -np.mean(np.sum(y *np.log(y_pred + 1e-8), axis=1))\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # Predict function\n",
        "    def predict(self, X):\n",
        "        y_pred = self.forward(X)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "\n",
        "    def predict_top_3(self, test_input):\n",
        "      y_pred = self.forward(test_input)\n",
        "      top_3_indices = np.argsort(y_pred, axis=1)[:, -3:][:, ::-1]  # Sort descending, get top 3\n",
        "      top_3_probabilities = np.sort(y_pred, axis=1)[:, -3:][:, ::-1]\n",
        "\n",
        "      result = []\n",
        "      for i in range(top_3_indices.shape[0]):\n",
        "        result.append({int(top_3_indices[i, j]): float(top_3_probabilities[i, j]) for j in range(3)})\n",
        "      print(result)\n",
        "      return result\n",
        "\n",
        "# Initialize and train the network\n",
        "input_size = 784  # Flattened image size\n",
        "hidden_sizes = [512, 256, 128, 64]  # Number of neurons in hidden layers\n",
        "output_size = 52 # Number of output classes\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "nn = NeuralNetwork(input_size, hidden_sizes, output_size)\n",
        "nn.train(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_test_pred = nn.predict(x_test)\n",
        "y_test_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_test_pred == y_test_true)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNHA3G8xXOvJ",
        "outputId": "20224f2d-17bb-4dcf-cb2e-acd076af9cb8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 3.9482\n",
            "Epoch 2/50, Loss: 3.9399\n",
            "Epoch 3/50, Loss: 3.8029\n",
            "Epoch 4/50, Loss: 3.0404\n",
            "Epoch 5/50, Loss: 1.9854\n",
            "Epoch 6/50, Loss: 1.4618\n",
            "Epoch 7/50, Loss: 1.2211\n",
            "Epoch 8/50, Loss: 1.0606\n",
            "Epoch 9/50, Loss: 0.9792\n",
            "Epoch 10/50, Loss: 0.9062\n",
            "Epoch 11/50, Loss: 0.8328\n",
            "Epoch 12/50, Loss: 0.7764\n",
            "Epoch 13/50, Loss: 0.7527\n",
            "Epoch 14/50, Loss: 0.7473\n",
            "Epoch 15/50, Loss: 0.7107\n",
            "Epoch 16/50, Loss: 0.6776\n",
            "Epoch 17/50, Loss: 0.6518\n",
            "Epoch 18/50, Loss: 0.6332\n",
            "Epoch 19/50, Loss: 0.5992\n",
            "Epoch 20/50, Loss: 0.6037\n",
            "Epoch 21/50, Loss: 0.6062\n",
            "Epoch 22/50, Loss: 0.5827\n",
            "Epoch 23/50, Loss: 0.5735\n",
            "Epoch 24/50, Loss: 0.5443\n",
            "Epoch 25/50, Loss: 0.5537\n",
            "Epoch 26/50, Loss: 0.5236\n",
            "Epoch 27/50, Loss: 0.5204\n",
            "Epoch 28/50, Loss: 0.5457\n",
            "Epoch 29/50, Loss: 0.5031\n",
            "Epoch 30/50, Loss: 0.5589\n",
            "Epoch 31/50, Loss: 0.5354\n",
            "Epoch 32/50, Loss: 0.4934\n",
            "Epoch 33/50, Loss: 0.4893\n",
            "Epoch 34/50, Loss: 0.5596\n",
            "Epoch 35/50, Loss: 0.4938\n",
            "Epoch 36/50, Loss: 0.4705\n",
            "Epoch 37/50, Loss: 0.5253\n",
            "Epoch 38/50, Loss: 0.4545\n",
            "Epoch 39/50, Loss: 0.4533\n",
            "Epoch 40/50, Loss: 0.4516\n",
            "Epoch 41/50, Loss: 0.4465\n",
            "Epoch 42/50, Loss: 0.4748\n",
            "Epoch 43/50, Loss: 0.4544\n",
            "Epoch 44/50, Loss: 0.4331\n",
            "Epoch 45/50, Loss: 0.4157\n",
            "Epoch 46/50, Loss: 0.4178\n",
            "Epoch 47/50, Loss: 0.4316\n",
            "Epoch 48/50, Loss: 0.4243\n",
            "Epoch 49/50, Loss: 0.3963\n",
            "Epoch 50/50, Loss: 0.3954\n",
            "Test Accuracy: 74.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = x_test[29]\n",
        "print(sample.shape)\n",
        "nn.predict(x_test[29])\n",
        "print(\"-----------\")\n",
        "\n",
        "nn.predict_top_3(x_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "BDrnfNQ2i1hQ",
        "outputId": "a26f2b28-6709-4d9b-d6a0-d06ef3939a43"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784,)\n",
            "-----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NeuralNetwork' object has no attribute 'predict_top_3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c9984d34bf37>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_top_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'predict_top_3'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(y_test[29])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9rNk4ici56c",
        "outputId": "814eb0a8-b550-49a2-ce3e-e10bf062d9fa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"letter_model.pkl\", \"wb\") as file:\n",
        "  pickle.dump(nn, file)"
      ],
      "metadata": {
        "id": "XB0sEz5WnyOZ"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}